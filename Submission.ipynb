{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/arsh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/arsh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from numpy.linalg import norm\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "WORD_LIST = stopwords.words('english')\n",
    "extra_words = ['hospital', 'patient','case', 'disease','laboratory', 'medical']\n",
    "WORD_LIST.extend(extra_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/train.dat','r') as f:\n",
    "    #next(f) # skip first row\n",
    "    train = pd.DataFrame(line.split('\\t') for line in f.readlines())\n",
    "train.columns = ['label', 'abstract']\n",
    "\n",
    "\n",
    "with open('./data/test.dat','r') as f:\n",
    "    #next(f) # skip first row\n",
    "    test = pd.DataFrame(line for line in f.readlines())\n",
    "test.columns = ['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for doc in train['abstract']:\n",
    "     data.append(doc)\n",
    "for doc in test['abstract']:\n",
    "     data.append(doc)\n",
    "\n",
    "data = np.array(data)\n",
    "\n",
    "\n",
    "labels = []\n",
    "\n",
    "for lab in train['label']:\n",
    "     labels.append(lab)\n",
    "\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    cleaned = []\n",
    "    for abstract in data:\n",
    "        cleaned.append(cleaner(abstract))\n",
    "    \n",
    "    vectored= []\n",
    "    for abstract in cleaned:\n",
    "        vectored.append(text_to_vector(abstract))\n",
    "    \n",
    "    stopped=[]\n",
    "    for abstract in vectored:\n",
    "        stopped.append(remove_stop_words(abstract))\n",
    "    \n",
    "    stemmed=[]\n",
    "    for abstract in stopped:\n",
    "        stemmed.append(stemmer(abstract))\n",
    "        \n",
    "    lemmatized=[]\n",
    "    for abstract in stemmed:\n",
    "        lemmatized.append(lemmatizer(abstract))\n",
    "        \n",
    "    lower = lower_case(lemmatized)\n",
    "    split = split_words(lower)\n",
    "    return split\n",
    "\n",
    "def cleaner(abstract):\n",
    "    clean= re.sub(re.compile('<.*?>'), '', abstract)\n",
    "    clean = re.sub(re.compile(r'\\d.*?\\d+'),'', clean)\n",
    "    clean = re.sub(ur\"[^\\w\\d'\\s]+\",' ',clean)\n",
    "    clean = re.sub(r'(^[ \\t]+|[ \\t]+(?=:))', '', clean, flags=re.M)\n",
    "    \n",
    "    return clean\n",
    "\n",
    "def stemmer(abstract):\n",
    "    port = PorterStemmer()\n",
    "    return \" \".join([port.stem(i) for i in abstract.split()])\n",
    "\n",
    "def lemmatizer(abstract):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    return \" \".join([wnl.lemmatize(i, 'v') for i in abstract.split()])\n",
    "\n",
    "def lower_case(data):\n",
    "    return [l.lower() for l in data]\n",
    "\n",
    "def split_words(data):\n",
    "    return [l.split() for l in data]\n",
    "\n",
    "def remove_stop_words(words):\n",
    "    result = []\n",
    "    for word in words:\n",
    "        if word not in WORD_LIST:\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "def text_to_vector(text):\n",
    "    words = WORD.findall(text)\n",
    "    words = [item.lower() for item in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-77ac09a8ef9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-53-01725a6dd6e7>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mstemmed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mabstract\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mstemmed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabstract\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mlemmatized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-01725a6dd6e7>\u001b[0m in \u001b[0;36mstemmer\u001b[0;34m(abstract)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabstract\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mabstract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabstract\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "data = preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'catheter',\n",
       " u'laboratori',\n",
       " u'event',\n",
       " u'and',\n",
       " u'hospit',\n",
       " u'outcom',\n",
       " u'with',\n",
       " u'direct',\n",
       " u'angioplasti',\n",
       " u'for',\n",
       " u'acut',\n",
       " u'myocardi',\n",
       " u'infarct',\n",
       " u'to',\n",
       " u'assess',\n",
       " u'the',\n",
       " u'safeti',\n",
       " u'of',\n",
       " u'direct',\n",
       " u'infarct',\n",
       " u'angioplasti',\n",
       " u'without',\n",
       " u'anteced',\n",
       " u'thrombolyt',\n",
       " u'therapi',\n",
       " u'catheter',\n",
       " u'laboratori',\n",
       " u'and',\n",
       " u'hospit',\n",
       " u'event',\n",
       " u'be',\n",
       " u'assess',\n",
       " u'in',\n",
       " u'consecut',\n",
       " u'treat',\n",
       " u'patient',\n",
       " u'with',\n",
       " u'infarct',\n",
       " u'involv',\n",
       " u'the',\n",
       " u'leave',\n",
       " u'anterior',\n",
       " u'descend',\n",
       " u'n',\n",
       " u'patient',\n",
       " u'right',\n",
       " u'n',\n",
       " u'and',\n",
       " u'circumflex',\n",
       " u'n',\n",
       " u'coronari',\n",
       " u'arteri',\n",
       " u'the',\n",
       " u'group',\n",
       " u'of',\n",
       " u'patient',\n",
       " u'be',\n",
       " u'similar',\n",
       " u'for',\n",
       " u'age',\n",
       " u'leave',\n",
       " u'anterior',\n",
       " u'descend',\n",
       " u'coronari',\n",
       " u'arteri',\n",
       " u'year',\n",
       " u'right',\n",
       " u'coronari',\n",
       " u'arteri',\n",
       " u'year',\n",
       " u'circumflex',\n",
       " u'coronari',\n",
       " u'arteri',\n",
       " u'year',\n",
       " u'patient',\n",
       " u'with',\n",
       " u'multivessel',\n",
       " u'diseas',\n",
       " u'leave',\n",
       " u'anterior',\n",
       " u'descend',\n",
       " u'coronari',\n",
       " u'arteri',\n",
       " u'right',\n",
       " u'coronari',\n",
       " u'arteri',\n",
       " u'circumflex',\n",
       " u'coronari',\n",
       " u'arteri',\n",
       " u'and',\n",
       " u'patient',\n",
       " u'with',\n",
       " u'initi',\n",
       " u'grade',\n",
       " u'antegrad',\n",
       " u'flow',\n",
       " u'leave',\n",
       " u'anterior',\n",
       " u'descend',\n",
       " u'coronari',\n",
       " u'arteri',\n",
       " u'right',\n",
       " u'coronari',\n",
       " u'arteri',\n",
       " u'circumflex',\n",
       " u'coronari',\n",
       " u'arteri',\n",
       " u'cardiogen',\n",
       " u'shock',\n",
       " u'wa',\n",
       " u'present',\n",
       " u'in',\n",
       " u'eight',\n",
       " u'patient',\n",
       " u'with',\n",
       " u'infarct',\n",
       " u'of',\n",
       " u'the',\n",
       " u'leave',\n",
       " u'anterior',\n",
       " u'descend',\n",
       " u'coronari',\n",
       " u'arteri',\n",
       " u'four',\n",
       " u'with',\n",
       " u'infarct',\n",
       " u'of',\n",
       " u'the',\n",
       " u'right',\n",
       " u'coronari',\n",
       " u'arteri',\n",
       " u'and',\n",
       " u'four',\n",
       " u'with',\n",
       " u'infarct',\n",
       " u'of',\n",
       " u'the',\n",
       " u'circumflex',\n",
       " u'coronari',\n",
       " u'arteri',\n",
       " u'major',\n",
       " u'catheter',\n",
       " u'laboratori',\n",
       " u'event',\n",
       " u'cardiovers',\n",
       " u'cardiopulmonari',\n",
       " u'resuscit',\n",
       " u'dopamin',\n",
       " u'or',\n",
       " u'intra',\n",
       " u'aortic',\n",
       " u'balloon',\n",
       " u'pump',\n",
       " u'support',\n",
       " u'for',\n",
       " u'hypotens',\n",
       " u'and',\n",
       " u'urgent',\n",
       " u'surgeri',\n",
       " u'occur',\n",
       " u'in',\n",
       " u'patient',\n",
       " u'with',\n",
       " u'infarct',\n",
       " u'of',\n",
       " u'the',\n",
       " u'leave',\n",
       " u'anterior',\n",
       " u'descend',\n",
       " u'coronari',\n",
       " u'arteri',\n",
       " u'eight',\n",
       " u'with',\n",
       " u'infarct',\n",
       " u'of',\n",
       " u'the',\n",
       " u'right',\n",
       " u'coronari',\n",
       " u'arteri',\n",
       " u'and',\n",
       " u'four',\n",
       " u'with',\n",
       " u'infarct',\n",
       " u'of',\n",
       " u'the',\n",
       " u'circumflex',\n",
       " u'coronari',\n",
       " u'arteri',\n",
       " u'of',\n",
       " u'shock',\n",
       " u'and',\n",
       " u'six',\n",
       " u'of',\n",
       " u'nonshock',\n",
       " u'patient',\n",
       " u'p',\n",
       " u'less',\n",
       " u'than',\n",
       " u'there',\n",
       " u'wa',\n",
       " u'one',\n",
       " u'in',\n",
       " u'laboratori',\n",
       " u'death',\n",
       " u'shock',\n",
       " u'patient',\n",
       " u'with',\n",
       " u'infarct',\n",
       " u'of',\n",
       " u'the',\n",
       " u'leave',\n",
       " u'anterior',\n",
       " u'descend',\n",
       " u'coronari',\n",
       " u'arteri']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterLen(docs, minlen):\n",
    "    r\"\"\" filter out terms that are too short. \n",
    "    docs is a list of lists, each inner list is a document represented as a list of words\n",
    "    minlen is the minimum length of the word to keep\n",
    "    \"\"\"\n",
    "    return [ [t.lower().encode('utf-8') for t in d if len(t) >= minlen ] for d in docs ]\n",
    "   \n",
    "data = filterLen(data ,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doppler',\n",
       " 'color',\n",
       " 'stenos',\n",
       " 'arteri',\n",
       " 'model',\n",
       " 'interpret',\n",
       " 'pattern',\n",
       " 'capabl',\n",
       " 'recent',\n",
       " 'introduc',\n",
       " 'doppler',\n",
       " 'color',\n",
       " 'devic',\n",
       " 'accur',\n",
       " 'detect',\n",
       " 'pattern',\n",
       " 'region',\n",
       " 'arteri',\n",
       " 'stenosi',\n",
       " 'evalu',\n",
       " 'vitro',\n",
       " 'model',\n",
       " 'pulsatil',\n",
       " 'simul',\n",
       " 'resist',\n",
       " 'vessel',\n",
       " 'induc',\n",
       " 'through',\n",
       " 'straight',\n",
       " 'acryl',\n",
       " 'which',\n",
       " 'altern',\n",
       " 'contain',\n",
       " 'axisymmetr',\n",
       " 'stenos',\n",
       " 'diamet',\n",
       " 'reduct',\n",
       " 'doppler',\n",
       " 'color',\n",
       " 'mapper',\n",
       " 'realtim',\n",
       " 'along',\n",
       " 'midplan',\n",
       " 'diamet',\n",
       " 'downstream',\n",
       " 'stenosi',\n",
       " 'comparison',\n",
       " 'doppler',\n",
       " 'color',\n",
       " 'result',\n",
       " 'similarli',\n",
       " 'record',\n",
       " 'visual',\n",
       " 'hydrogen',\n",
       " 'bubbl',\n",
       " 'close',\n",
       " 'correspond',\n",
       " 'featur',\n",
       " 'includ',\n",
       " 'detect',\n",
       " 'veloc',\n",
       " 'centerlin',\n",
       " 'separ',\n",
       " 'distinct',\n",
       " 'pattern',\n",
       " 'exist',\n",
       " 'stenot',\n",
       " 'these',\n",
       " 'should',\n",
       " 'consider',\n",
       " 'diagnos',\n",
       " 'clinic',\n",
       " 'diseas',\n",
       " 'condit']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1210]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These our the function which we will use to build csr matrix, normalize them , view csr matrix info. We will pass our review data to build a normalize csr_matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [nrows 28880, ncols 26498, nnz 1468001]\n"
     ]
    }
   ],
   "source": [
    "def build_matrix(docs):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    nrows = len(docs)\n",
    "    idx = {}\n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "    for d in docs:\n",
    "        nnz += len(set(d))\n",
    "        for w in d:\n",
    "            if w not in idx:\n",
    "                idx[w] = tid\n",
    "                tid += 1\n",
    "    ncols = len(idx)\n",
    "        \n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        cnt = Counter(d)\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):\n",
    "            ind[j+n] = idx[k]\n",
    "            val[j+n] = cnt[k]\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat\n",
    "\n",
    "def csr_info(mat, name=\"\", non_empy=False):\n",
    "    r\"\"\" Print out info about this CSR matrix. If non_empy, \n",
    "    report number of non-empty rows and cols as well\n",
    "    \"\"\"\n",
    "    if non_empy:\n",
    "        print(\"%s [nrows %d (%d non-empty), ncols %d (%d non-empty), nnz %d]\" % (\n",
    "                name, mat.shape[0], \n",
    "                sum(1 if mat.indptr[i+1] > mat.indptr[i] else 0 \n",
    "                for i in range(mat.shape[0])), \n",
    "                mat.shape[1], len(np.unique(mat.indices)), \n",
    "                len(mat.data)))\n",
    "    else:\n",
    "        print( \"%s [nrows %d, ncols %d, nnz %d]\" % (name, \n",
    "                mat.shape[0], mat.shape[1], len(mat.data)) )\n",
    "\n",
    "def csr_l2normalize(mat, copy=False, **kargs):\n",
    "    r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm. \n",
    "    If copy is True, returns a copy of the normalized matrix.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            \n",
    "    if copy is True:\n",
    "        return mat\n",
    "\n",
    "def csr_idf(mat, copy=False, **kargs):\n",
    "    r\"\"\" Scale a CSR matrix by idf. \n",
    "    Returns scaling factors as dict. If copy is True, \n",
    "    returns scaled matrix and scaling factors.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # document frequency\n",
    "    df = defaultdict(int)\n",
    "    for i in ind:\n",
    "        df[i] += 1\n",
    "    # inverse document frequency\n",
    "    for k,v in df.items():\n",
    "        df[k] = np.log(nrows / float(v))  ## df turns to idf - reusing memory\n",
    "    # scale by idf\n",
    "    for i in range(0, nnz):\n",
    "        val[i] *= df[ind[i]]\n",
    "        \n",
    "    return df if copy is False else mat\n",
    "\n",
    "sparse_matrix = build_matrix(data)\n",
    "csr_info(sparse_matrix)\n",
    "\n",
    "\n",
    "sparse_matrix = csr_idf(sparse_matrix, copy=True)\n",
    "sparse_matrix = csr_l2normalize(sparse_matrix, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(mat, cls, fold=1, d=10):\n",
    "    r\"\"\" Split the matrix and class info into train and test data using d-fold hold-out\n",
    "    \"\"\"\n",
    "    n = mat.shape[0]\n",
    "    r = int(np.ceil(n*1.0/d))\n",
    "    mattr = []\n",
    "    clstr = []\n",
    "    # split mat and cls into d folds\n",
    "    for f in range(d):\n",
    "        if f+1 != fold:\n",
    "            mattr.append( mat[f*r: min((f+1)*r, n)] )\n",
    "            clstr.extend( cls[f*r: min((f+1)*r, n)] )\n",
    "    # join all fold matrices that are not the test matrix\n",
    "    train = sp.vstack(mattr, format='csr')\n",
    "    # extract the test matrix and class values associated with the test rows\n",
    "    test = mat[(fold-1)*r: min(fold*r, n), :]\n",
    "    clste = cls[(fold-1)*r: min(fold*r, n)]\n",
    "\n",
    "    return train, clstr, test, clste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building csr matrix, we will divide it into train_mat and test_mat, so that our matrix our of same dimensions , which is required for matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat = sparse_matrix[0:14438]\n",
    "test_mat =  sparse_matrix[14438:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calculate cosine similarity between train_mat and test_mat.Due to machine incompetency, we use batch size of 10 to calculate similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, clstr, test, clste = splitData(train_mat, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_n_space(m1, m2, batch_size=10):\n",
    "    assert m1.shape[1] == m2.shape[1]\n",
    "    ret = np.ndarray((m1.shape[0], m2.shape[0]))\n",
    "    for row_i in range(0, int(m1.shape[0] / batch_size) + 1):\n",
    "        start = row_i * batch_size\n",
    "        end = min([(row_i + 1) * batch_size, m1.shape[0]])\n",
    "        if end <= start:\n",
    "            break \n",
    "        rows = m1[start: end]\n",
    "        sim = cosine_similarity(rows, m2) # rows is O(1) size\n",
    "        ret[start: end] = sim\n",
    "    return ret\n",
    "\n",
    "cosineSimilarityValue = cosine_similarity_n_space(test_mat,train_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14442, 14438)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosineSimilarityValue.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions= []\n",
    "for row in cosineSimilarityValue:\n",
    "\n",
    "    \n",
    "    k=200\n",
    "    partitioned_row_byindex = np.argpartition(-row, k)  \n",
    "    similar_index = partitioned_row_byindex[:k]\n",
    "\n",
    "    \n",
    "    type1 = 0\n",
    "    type2 = 0\n",
    "    type3 = 0\n",
    "    type4 = 0\n",
    "    type5 = 0\n",
    "\n",
    "    \n",
    "    for index in similar_index:\n",
    "        \n",
    "        if(labels[index] == '1'):\n",
    "            type1+=1\n",
    "        elif(labels[index] == '2'):\n",
    "            type2+=1\n",
    "        elif(labels[index] == '3'):\n",
    "            type3+=1\n",
    "        elif(labels[index] == '4'):\n",
    "            type4+=1\n",
    "        elif(labels[index] == '5'):\n",
    "            type5+=1 \n",
    "     \n",
    "    vote = max(type1,type2,type3,type4,type5)\n",
    "    \n",
    "    if vote == type1:\n",
    "        predictions.append(1)\n",
    "    elif vote == type2:\n",
    "        predictions.append(2)\n",
    "    elif vote == type3:\n",
    "        predictions.append(3)\n",
    "    elif vote == type4:\n",
    "        predictions.append(4)\n",
    "    elif vote == type5:\n",
    "        predictions.append(5)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./format.dat', 'w')\n",
    "for prediction in predictions:\n",
    "    print >>f, prediction\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
